{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import time\n",
    "import pathlib\n",
    "import imageio\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from torchvision.datasets import CIFAR10, STL10\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data as utils\n",
    "import visdom\n",
    "from torchvision import transforms\n",
    "from GAN_training.models import resnet, resnet_extra, resnet_48\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optclass:\n",
    "    workaround = True\n",
    "    \n",
    "xp = '/scratch0/ilya/locDoc/ACGAN/experiments/'\n",
    "    \n",
    "opt = optclass()\n",
    "optdict = {\n",
    "    #'outf': '/scratch0/ilya/locDoc/ACGAN/experiments/yogesh_acgan_0p2',\n",
    "    'outf': xp+'marygan-stl-48-miyato-hyp-lrGp4-auxp4',\n",
    "    'netG': xp+'marygan-stl-48-miyato-hyp-lrGp4-auxp4/netG_iter_069999.pth',\n",
    "    'marygan': True,\n",
    "    'imageSize': 48,\n",
    "    'data_root': '/scratch0/ilya/locDoc/data/stl10',\n",
    "    'dataset': 'cifar',\n",
    "    'dev_batch_size': 100,\n",
    "    'size_labeled_data': 4000,\n",
    "    'train_batch_size': 128,\n",
    "    'train_batch_size_2': 100,\n",
    "    'cifar_fname': '/scratch0/ilya/locDoc/data/cifar10/fid_is_scores.npz',\n",
    "    'nz': 128,\n",
    "    'GAN_nz': 128,\n",
    "    'ngpu': 1,\n",
    "    'nc':3,\n",
    "    'num_classes': 10\n",
    "}\n",
    "for k, v in optdict.items():\n",
    "    setattr(opt, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three celebA networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'celeb_cpy/celeba_vanillagan'\n",
    "opt.netG = opt.outf+'/netG_iter_129999.pth' # 4.130121811844333\n",
    "opt.marygan = False\n",
    "opt.imageSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'celeb_cpy/celeba5c_marygan'\n",
    "opt.netG = opt.outf+'/netG_iter_129999.pth' # 3.6509132644800673\n",
    "opt.marygan = True\n",
    "opt.imageSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'celeb_cpy/celeba5c_acgan'\n",
    "opt.netG = opt.outf+'/netG_iter_119999.pth' # 5.074366134380284 \n",
    "opt.marygan = False\n",
    "opt.imageSize = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three STL networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'marygan-stl-48-miyato-hyp-lrGp4-auxp4'\n",
    "opt.netG = opt.outf+'/netG_iter_069999.pth'\n",
    "opt.marygan = True\n",
    "opt.imageSize = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'vanillagan-stl-48-miyato-hyp-lrGp4'\n",
    "opt.netG = opt.outf+'/netG_iter_089999.pth'\n",
    "opt.marygan = False\n",
    "opt.imageSize = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'yogesh-acgan-stl-48-miyato-hyp-lrGp4'\n",
    "opt.netG = opt.outf+'/netG_iter_089999.pth'\n",
    "opt.marygan = False\n",
    "opt.imageSize = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three CIFAR networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'yogesh_marygan_0p2'\n",
    "# opt.netG = opt.outf+'/netG_iter_399999.pth'\n",
    "opt.netG = opt.outf+'/netG_iter_399999.pth'\n",
    "opt.marygan = True\n",
    "opt.imageSize = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'yogesh_acgan_0p2'\n",
    "opt.netG = opt.outf+'/netG_iter_449999.pth'\n",
    "opt.marygan = False\n",
    "opt.imageSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.outf = xp+'yogesh_vanillagan_cifar'\n",
    "opt.netG = opt.outf+'/netG_iter_489999.pth' # 14.711713683439996 8.034166\n",
    "opt.marygan = False\n",
    "opt.imageSize = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create images with the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.netG == '':\n",
    "    netGfiles = glob.glob(os.path.join(opt.outf, 'netG_iter_*.pth'))\n",
    "    netGfiles.sort(key = lambda s: int(s.split('_')[-1].split('.')[0]))\n",
    "    opt.netG = netGfiles[-1]\n",
    "    print(opt.netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:112: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.dense.weight.data, 1.)\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:113: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.final.weight.data, 1.)\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:16: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n"
     ]
    }
   ],
   "source": [
    "if opt.imageSize == 32:\n",
    "    netG = resnet.Generator(opt)\n",
    "elif opt.imageSize == 64:\n",
    "    netG = resnet_extra.Generator(opt)\n",
    "elif opt.imageSize == 48:\n",
    "    netG = resnet_48.Generator(opt)\n",
    "netG.load_state_dict(torch.load(opt.netG))\n",
    "netG = netG.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1179554094045b1ad5c6c37d7e5424e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Generating', max=8, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "batch_size = opt.train_batch_size\n",
    "nz = opt.nz\n",
    "noise = torch.FloatTensor(opt.train_batch_size, nz)\n",
    "noise = noise.cuda()\n",
    "num_classes = 10\n",
    "klass_picked = None\n",
    "\n",
    "# create images\n",
    "n_used_imgs = 1000\n",
    "n_gen_imgs = ((n_used_imgs // opt.train_batch_size) + 1) * opt.train_batch_size\n",
    "x = np.empty((n_gen_imgs,3,opt.imageSize,opt.imageSize), dtype=np.uint8)\n",
    "save_noise = np.empty((n_gen_imgs,128))\n",
    "# create a bunch of GAN images\n",
    "for l in  tqdm_notebook(range((n_used_imgs // opt.train_batch_size) + 1),desc='Generating'):\n",
    "    start = l * opt.train_batch_size\n",
    "    end = start + opt.train_batch_size\n",
    "    noise.data.resize_(batch_size, nz).normal_(0, 1)\n",
    "    #label = np.random.randint(0, num_classes, batch_size)\n",
    "    if klass_picked is None:\n",
    "        label = np.random.randint(0, num_classes, batch_size)\n",
    "    else:\n",
    "        label = np.ones((batch_size,),dtype=int)*klass_picked\n",
    "    noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "    if not opt.marygan:\n",
    "        class_onehot = np.zeros((batch_size, num_classes))\n",
    "        class_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
    "    save_noise[start:end] = noise_\n",
    "    noise_ = (torch.from_numpy(noise_))\n",
    "    noise.data.copy_(noise_.view(batch_size, nz))\n",
    "    fake = netG(noise).data.cpu().numpy()\n",
    "    fake = np.floor((fake + 1) * 255/2.0).astype(np.uint8)\n",
    "    x[start:end] = fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with class filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FYI: Load `compnet` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a5fa26e9104a62a4cb5ece3723efb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4096), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "batch_size = opt.train_batch_size\n",
    "nz = opt.nz\n",
    "noise = torch.FloatTensor(opt.train_batch_size, nz)\n",
    "noise = noise.cuda()\n",
    "num_classes = 10\n",
    "klass_picked = 9\n",
    "comp_img_size = 32\n",
    "# comp_img_size = opt.imageSize\n",
    "\n",
    "disc_batch = torch.FloatTensor(opt.train_batch_size, 3, comp_img_size, comp_img_size)\n",
    "disc_batch = disc_batch.cuda()\n",
    "\n",
    "# create images\n",
    "n_used_imgs = 4000\n",
    "n_gen_imgs = ((n_used_imgs // opt.train_batch_size) + 1) * opt.train_batch_size\n",
    "x = np.empty((n_gen_imgs,3,opt.imageSize,opt.imageSize), dtype=np.uint8)\n",
    "batch_in = np.empty((batch_size,3,comp_img_size,comp_img_size))\n",
    "# create a bunch of GAN images\n",
    "start = 0\n",
    "pbar = tqdm_notebook(total=n_gen_imgs)\n",
    "while not start == n_gen_imgs:\n",
    "    \n",
    "#for l in  tqdm(range((n_used_imgs // opt.train_batch_size) + 1),desc='Generating'):\n",
    "    noise.data.resize_(batch_size, nz).normal_(0, 1)\n",
    "    #label = np.random.randint(0, num_classes, batch_size)\n",
    "    if klass_picked is None:\n",
    "        label = np.random.randint(0, num_classes, batch_size)\n",
    "    else:\n",
    "        label = np.ones((batch_size,),dtype=int)*klass_picked\n",
    "    noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "    if not opt.marygan:\n",
    "        class_onehot = np.zeros((batch_size, num_classes))\n",
    "        class_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
    "    noise_ = (torch.from_numpy(noise_))\n",
    "    noise.data.copy_(noise_.view(batch_size, nz))\n",
    "    fake = netG(noise).data.cpu().numpy()\n",
    "    fake = np.floor((fake + 1) * 255/2.0).astype(np.uint8)\n",
    "    \n",
    "    for bi in range(fake.shape[0]):\n",
    "        batch_in[bi] = minimal_trans(np.moveaxis(fake[bi],0,-1))\n",
    "    disc_batch.data.copy_(torch.from_numpy(batch_in));\n",
    "    batch_out = compnet(disc_batch).detach().data.cpu()\n",
    "    class_filt = (np.argmax(batch_out,1) == klass_picked).numpy().astype(bool)\n",
    "    fake_picked = fake[class_filt]\n",
    "    end = min(start + fake_picked.shape[0], n_gen_imgs)\n",
    "    \n",
    "    x[start:end] = fake_picked[:(end-start)]\n",
    "    start = end\n",
    "    pbar.update(fake_picked.shape[0])\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with face confidence filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6df6fcab9394606aa511462c212db2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=128), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = opt.train_batch_size\n",
    "nz = opt.nz\n",
    "noise = torch.FloatTensor(opt.train_batch_size, nz)\n",
    "noise = noise.cuda()\n",
    "num_classes = 10\n",
    "klass_picked = 8\n",
    "comp_img_size = 64\n",
    "# comp_img_size = opt.imageSize\n",
    "\n",
    "disc_batch = torch.FloatTensor(opt.train_batch_size, 3, comp_img_size, comp_img_size)\n",
    "disc_batch = disc_batch.cuda()\n",
    "\n",
    "# create images\n",
    "n_used_imgs = 100\n",
    "min_conf = 0.99\n",
    "n_gen_imgs = ((n_used_imgs // opt.train_batch_size) + 1) * opt.train_batch_size\n",
    "x = np.empty((n_gen_imgs,3,opt.imageSize,opt.imageSize), dtype=np.uint8)\n",
    "batch_in = np.empty((batch_size,3,comp_img_size,comp_img_size))\n",
    "# create a bunch of GAN images\n",
    "start = 0\n",
    "pbar = tqdm_notebook(total=n_gen_imgs)\n",
    "while not start == n_gen_imgs:\n",
    "    \n",
    "#for l in  tqdm(range((n_used_imgs // opt.train_batch_size) + 1),desc='Generating'):\n",
    "    noise.data.resize_(batch_size, nz).normal_(0, 1)\n",
    "    #label = np.random.randint(0, num_classes, batch_size)\n",
    "    if klass_picked is None:\n",
    "        label = np.random.randint(0, num_classes, batch_size)\n",
    "    else:\n",
    "        label = np.ones((batch_size,),dtype=int)*klass_picked\n",
    "    noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "    if not opt.marygan:\n",
    "        class_onehot = np.zeros((batch_size, num_classes))\n",
    "        class_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
    "    noise_ = (torch.from_numpy(noise_))\n",
    "    noise.data.copy_(noise_.view(batch_size, nz))\n",
    "    fake = netG(noise).data.cpu().numpy()\n",
    "    fake = np.floor((fake + 1) * 255/2.0).astype(np.uint8)\n",
    "    \n",
    "    class_filt = np.zeros(batch_size).astype(bool)\n",
    "    for bi in range(fake.shape[0]):\n",
    "        res = detector.detect_faces(np.moveaxis(fake[bi],0,-1))\n",
    "        if not res:\n",
    "#         if res and res[0]['confidence'] > min_conf:\n",
    "            class_filt[bi] = True\n",
    "        \n",
    "    fake_picked = fake[class_filt]\n",
    "    end = min(start + fake_picked.shape[0], n_gen_imgs)\n",
    "    \n",
    "    x[start:end] = fake_picked[:(end-start)]\n",
    "    start = end\n",
    "    pbar.update(fake_picked.shape[0])\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb6dba6e5f14965bdc894c82392f103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef905535c0>"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(x), nrow=int(np.sqrt(n_used_imgs)), padding=0, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5243ac68dbcc447ab9b6af36e609ab29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f36840685c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.moveaxis(x[0],0,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.netD = opt.netG.replace('netG', 'netD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:84: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n",
      "/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/GAN_training/models/resnet.py:64: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n"
     ]
    }
   ],
   "source": [
    "if opt.imageSize == 32:\n",
    "    netD = resnet.Discriminator(opt)\n",
    "elif opt.imageSize == 64:\n",
    "    netD = resnet_extra.Discriminator(opt)\n",
    "elif opt.imageSize == 48:\n",
    "    netD = resnet_48.Discriminator(opt)\n",
    "else:\n",
    "    raise NotImplementedError('A network for imageSize %i is not implemented!' % opt.imageSize)\n",
    "\n",
    "netD.load_state_dict(torch.load(opt.netD))\n",
    "netD = netD.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidenote: Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 1687 // 64 + int((np.sqrt(n_used_imgs)))*(1292 // 64)\n",
    "#sec = 1839 // 64 + int(np.sqrt(n_used_imgs))*(497 // 64)-1\n",
    "sec = first+1\n",
    "\n",
    "Ni = 10\n",
    "noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "noise_[:Ni] = np.linspace(save_noise[first],save_noise[sec],Ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noise_ = (torch.from_numpy(noise_))\n",
    "noise.data.copy_(noise_.view(batch_size, nz))\n",
    "interp = netG(noise).data.cpu().numpy()\n",
    "interp = np.floor((interp + 1) * 255/2.0).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d6592cb33948378973d29e98ca90f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe45710c5f8>"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(interp[:20]), nrow=int(20), padding=2, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "imageio.imsave('~/marygan.png', np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "### go beyond the boundar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_ = noise_.numpy()\n",
    "d = (save_noise[sec]-save_noise[first]) / 10.0\n",
    "further1 = save_noise[first] + (d * np.expand_dims(np.arange(20),1))\n",
    "noise_[:20] = further1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.551881954370094"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "further1.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load feature extracting network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.models.vgg5 import VGG\n",
    "compnet = VGG('VGG19')\n",
    "compnet = torch.nn.DataParallel(compnet)\n",
    "checkpoint = torch.load(os.path.join('/scratch0/ilya/locDoc/classifiers/vgg16','ckpt_200.t7'))\n",
    "compnet.load_state_dict(checkpoint['net'])\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval();\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar,32\n",
    "from classification.models.densenet import DenseNet121\n",
    "compnet = DenseNet121()\n",
    "compnet = torch.nn.DataParallel(compnet)\n",
    "checkpoint = torch.load(os.path.join('/scratch0/ilya/locDoc/classifiers/densenet121','ckpt_47.t7'))\n",
    "compnet.load_state_dict(checkpoint['net'])\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval();\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "minimal_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.models.vgg_official2 import vgg16\n",
    "compnet = vgg16(pretrained=True)\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval()\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.models.vgg_face_dag import vgg_face_dag\n",
    "compnet = vgg_face_dag(weights_path='/scratch0/ilya/locDownloads/vgg_face_dag.pth')\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval()\n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "                           transforms.Resize((224,224)),\n",
    "                           transforms.Lambda(lambda img: np.moveaxis(np.array(img),-1,0) ),\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stl, 64\n",
    "from classification.models.densenet import densenet_stl as DenseNet121\n",
    "compnet = DenseNet121()\n",
    "compnet = torch.nn.DataParallel(compnet)\n",
    "checkpoint = torch.load(os.path.join('/scratch0/ilya/locDoc/classifiers/stl/densenet121_64/ckpt.t7'))\n",
    "compnet.load_state_dict(checkpoint['net'])\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval();\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "minimal_trans = transform_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass imgs through net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb320d6c7a914dbda8124e633ad749bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Preprocess', max=1024, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#net_in = np.empty((x.shape[0],3,64,64)) # stl\n",
    "net_in = np.empty((x.shape[0],3,32,32))\n",
    "#net_in = np.empty((x.shape[0],) + (3,224,224)) # celebA, vgg-official\n",
    "for i in tqdm_notebook(range(x.shape[0]),desc='Preprocess'):\n",
    "    net_in[i] = transform_test(np.moveaxis(x[i],0,-1))\n",
    "\n",
    "my_dataset = utils.TensorDataset(torch.FloatTensor(net_in))\n",
    "my_dataloader = utils.DataLoader(my_dataset, batch_size=opt.train_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44cfe0c56344b0dacfe591b8403c4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Extract Feat', max=8, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_out = np.empty(x.shape[0])\n",
    "logit_out = np.empty((x.shape[0],10))\n",
    "#net_out = np.empty((x.shape[0], 602112))\n",
    "#net_out = np.empty((x.shape[0], 12288))\n",
    "net_out = np.empty((x.shape[0], 1024)) # Densenet\n",
    "#net_out = np.empty((x.shape[0], 2622)) # vgg-face\n",
    "#net_out = np.empty((x.shape[0], 25088)) # vgg official penultimate\n",
    "for i, batch in enumerate(tqdm_notebook(my_dataloader,desc='Extract Feat')):\n",
    "    start = i * opt.train_batch_size\n",
    "    end = start + opt.train_batch_size\n",
    "    batch_in = batch[0].to(device)\n",
    "    #batch_out = compnet(batch_in).detach().data.cpu() # faces\n",
    "    batch_out = compnet(batch_in, feat=1).detach().data.cpu() # vgg official\n",
    "    net_out[start:end] = batch_out\n",
    "    logit_out[start:end] = compnet(batch_in).detach().data.cpu()\n",
    "    class_out[start:end] = logit_out[start:end].argmax(1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OR... MSE style\n",
    "mse_out = x.reshape(x.shape[0], np.prod(x.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/mtcnn/layer_factory.py:221: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/cfarhomes/ilyak/Screenshot from 2019-05-19 18-05-24.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidImage",
     "evalue": "Image not valid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidImage\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f0b5430ad8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/mtcnn/mtcnn.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image not valid.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidImage\u001b[0m: Image not valid."
     ]
    }
   ],
   "source": [
    "res = detector.detect_faces(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box': [1, -4, 60, 71],\n",
       "  'confidence': 0.9998058676719666,\n",
       "  'keypoints': {'left_eye': (17, 25),\n",
       "   'right_eye': (46, 24),\n",
       "   'nose': (30, 39),\n",
       "   'mouth_left': (20, 54),\n",
       "   'mouth_right': (45, 53)}}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.detect_faces(np.moveaxis(x[488],0,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot closest pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = pairwise_distances(net_out)\n",
    "D2 = pairwise_distances(mse_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = (D1 - D1.mean()) / D1.std()\n",
    "D2 = (D2 - D2.mean()) / D2.std()\n",
    "D = D1+ 0.25*D2\n",
    "deleted_flag = D.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OR just net_out\n",
    "D = pairwise_distances(net_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OR just net_out\n",
    "D = pairwise_distances(mse_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the diagonal and lower triangle\n",
    "to_del = np.tril(np.ones((D.shape[0], D.shape[0]), dtype=int))\n",
    "D[to_del == 1] = D.max()\n",
    "\n",
    "\n",
    "dists = D.flatten()\n",
    "closest_N = 40\n",
    "idxs = np.argpartition(dists,closest_N)\n",
    "min_idxs = sorted(idxs[:closest_N], key=lambda i: dists[i])\n",
    "#print(dists[min_idxs])\n",
    "closest_idxs = [(idx // D.shape[0], idx % D.shape[0]) for idx in min_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_imgs = np.zeros((closest_N * 2,)+x.shape[1:])\n",
    "used_idxs = []\n",
    "l = 0\n",
    "for (i,j) in closest_idxs:\n",
    "    #res1 = detector.detect_faces(np.moveaxis(x[i],0,-1))\n",
    "    #res2 = detector.detect_faces(np.moveaxis(x[j],0,-1))\n",
    "    if (i not in used_idxs) and (j not in used_idxs):# and res1 and res2:\n",
    "        closest_imgs[2*l] = x[min(i,j)]\n",
    "        closest_imgs[2*l + 1] = x[max(i,j)]\n",
    "        used_idxs.append(i)\n",
    "        used_idxs.append(j)\n",
    "        l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f9b0ebfb61458d93aa8426118dcb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe74dd447f0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot closest pairs\n",
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(closest_imgs[:20]), nrow=2, padding=0, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "imageio.imsave('~/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/figs/test.png', np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[460, 550, 710, 846, 222, 1441, 629, 1720, 488, 1652]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_idxs\n",
    "#closest_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55ef57402b149aa81a97dc5fbecaf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3608081d68>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(closest_imgs[20:]), nrow=2, padding=0, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save selections manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = []\n",
    "row2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "aidx = 8\n",
    "row1.append(closest_imgs[2*aidx])\n",
    "row2.append(closest_imgs[2*aidx + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cf5914a6c1467f80ff5b3f3102b938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9490ad89e8>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(row1 + row2), nrow=len(row1), padding=0, normalize=True)\n",
    "fake_grid_img = np.moveaxis(fake_grid.data.cpu().numpy(),0,-1)\n",
    "plt.imshow(fake_grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "imageio.imsave('~/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/figs/celebA_vanilla_10.png', fake_grid_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load diversity statistics that a script output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = '/scratch0/ilya/locDoc/ACGAN/experiments/diversity_plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = '/fs/vulcan-scratch/ilyak/locDoc/experiments/diversity_plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath += 'celeba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/fs/vulcan-scratch/ilyak/locDoc/experiments/diversity_plots/celeba/vanilla5.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-729d81e6502d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvanilla_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'vanilla5.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmarygan_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'marygan5.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0macgan_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'acgan5.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/fs/vulcan-scratch/ilyak/locDoc/experiments/diversity_plots/celeba/vanilla5.npz'"
     ]
    }
   ],
   "source": [
    "vanilla_data = np.load(fpath + 'vanilla5.npz')\n",
    "marygan_data = np.load(fpath + 'marygan5.npz')\n",
    "acgan_data = np.load(fpath + 'acgan5.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, vanilla_data['mins'].shape[1]*128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b647e38964d440739d81ee45bee8fcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'GANs on CelebA')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "def show(color, data):\n",
    "    y = data.mean(0)\n",
    "    err = data.std(0)\n",
    "    plt.plot(t, y, 'k-', color=color)\n",
    "    plt.fill_between(t, y-err, y+err, facecolor=color, alpha=0.3)\n",
    "    \n",
    "show('orange', marygan_data['mins'])\n",
    "show('green', acgan_data['mins'])\n",
    "show('blue', vanilla_data['mins'])\n",
    "\n",
    "plt.legend(['marygan 5 class', 'acgan 5 class', 'vanilla'])\n",
    "plt.ylabel('Min VGG-Face Embedding distance')\n",
    "plt.xlabel('n generated examples')\n",
    "plt.title('GANs on CelebA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Confidence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch0/ilya/locDoc/ACGAN/experiments/confidence_plots/marygan_genW_10001_trial_0.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56baf8c40d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'marygan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acgan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vanilla'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp_rama\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'confidence_plots/%s_genW_10001_trial_0.npz'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'confs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfidx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp_rama\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'confidence_plots/%s_genW_10001_trial_%i.npz'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'confs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch0/ilya/locDoc/ACGAN/experiments/confidence_plots/marygan_genW_10001_trial_0.npz'"
     ]
    }
   ],
   "source": [
    "xp_rama = '/scratch0/ilya/locDoc/ACGAN/experiments/'\n",
    "datas = []\n",
    "for mode in ['marygan', 'acgan', 'vanilla']:\n",
    "    data = np.load(xp_rama+'confidence_plots/%s_genW_10001_trial_0.npz' % mode)['confs']\n",
    "    for fidx in range(1,10):\n",
    "        data = np.concatenate([data, np.load(xp_rama+'confidence_plots/%s_genW_10001_trial_%i.npz' % (mode, fidx) )['confs']])\n",
    "    datas.append(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51b7c510a11406c8ac18617d88d0848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybins = np.array([0, 0.9, 0.99,0.999,0.9999])\n",
    "x_pos = np.arange(len(mybins))\n",
    "offset = [-0.3, 0, 0.3]\n",
    "colors = ['green', 'orange', 'blue']\n",
    "for i, d in enumerate(datas):\n",
    "    bt = (np.expand_dims(d,-1) > np.expand_dims(mybins,0)).mean(axis=0)\n",
    "    ax.bar(x_pos + offset[i], bt, 0.3, color=colors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(67.31944444444443, 0.5, 'Percent of Generator Outputs')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax.legend(['Mode GAN', 'ACGAN', 'SN GAN'])\n",
    "ax.axes.set_xticklabels([''] + ['> %.4f' % mybins[i] for i in range(len(mybins))])\n",
    "ax.set_xlabel('MTCNN face detection confidence')\n",
    "ax.set_ylabel('Percent of Generator Outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.axes.set_yticks(np.arange(0,1,0.05));\n",
    "labels = [(('%.2f' % t) if (i % 2 == 0) else '') for (i,t) in enumerate(np.arange(0,1,0.05))]\n",
    "ax.axes.set_yticklabels(labels);\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/figs/celeba_confidence.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face sorting by score batch plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c5dfacaa8cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpamt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpamt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'edge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x_imgs = x[:100]\n",
    "confs = np.zeros(100)\n",
    "pamt = 8\n",
    "for bi in range(100):\n",
    "    img = np.pad(np.moveaxis(x[bi],0,-1), ((pamt,pamt),(pamt,pamt),(0,0)), 'edge')\n",
    "    res = detector.detect_faces(img)\n",
    "    if res:\n",
    "        confs[bi] = res[0]['confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_order = np.argsort(-confs)\n",
    "conf_square = confs[conf_order].reshape((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sorted = x_imgs[conf_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3bf904efa849589a74c2c627977958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff09c16f7b8>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_grid = vutils.make_grid(torch.Tensor(x_sorted), nrow=10, padding=0, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.axes.set_xticks(32 + np.arange(0,10) * 64);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.axes.set_yticks(32 + np.arange(0,10) * 64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.axes.set_xticklabels(['%.3f' % c for c in confs[conf_order][90:]], rotation=-45);\n",
    "ax.axes.set_yticklabels([('%.10f' % c)[:6] for c in conf_square[:,0]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/figs/celeba_maryg_batch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst face plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c12e6b6f9ff4221b29c200a2dae3646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef8b427d68>"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(x), nrow=int(np.sqrt(n_used_imgs)), padding=0, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3, 64, 64)"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = [120,58,47]\n",
    "print(len(selection))\n",
    "x[selection].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be53af8b9ab24b52bf737695f3b682c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef8b2f7400>"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_grid = vutils.make_grid(torch.Tensor(x[selection]), nrow=10, padding=0, normalize=True)\n",
    "fake_img = np.moveaxis(fake_grid.data.cpu().numpy(),0,-1)\n",
    "plt.imshow(fake_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "imageio.imsave('/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/figs/celeba_marygan_stretch_bad_eg2.png', fake_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "imageio.imsave('/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/figs/celeba_celeba_bad_eg.png', fake_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_fitter = TSNE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embed = tsne_fitter.fit_transform(net_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = tsne_embed[:,0], tsne_embed[:,1]\n",
    "tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdfa058c2674e14bad8c98a840cba4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fef91311e80>"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(tx,ty,c=class_out.astype(int), alpha=0.4,  cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "width = 4000\n",
    "height = 3000\n",
    "max_dim = 100.\n",
    "full_image = Image.new('RGB', (width, height))\n",
    "for idx in range(x.shape[0]):\n",
    "    tile = Image.fromarray(np.moveaxis(x[idx],0,-1))\n",
    "    rs = max(1, tile.width / max_dim, tile.height / max_dim)\n",
    "    tile = tile.resize((int(tile.width / rs),\n",
    "                        int(tile.height / rs)),\n",
    "                       Image.ANTIALIAS)\n",
    "    full_image.paste(tile, (int((width-max_dim) * tx[idx]),\n",
    "                            int((height-max_dim) * ty[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef913d3f28>"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.imsave('/scratch0/ilya/locDownloads/stl_tsne.png', full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7e509e2d0446c69036e625a08fd83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches sorted by one of the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_out = np.abs(np.expand_dims(logit_out.max(1),-1) - logit_out).sum(1)\n",
    "ord_idx = np.argsort(-conf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just natural order\n",
    "ord_idx = np.arange(0,x.shape[0],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fakes = np.zeros((10,width,3,x.shape[-1],x.shape[-1]))\n",
    "idx_p = np.zeros(width,dtype=int)\n",
    "for idx in ord_idx:\n",
    "    lab = int(class_out[idx])\n",
    "    insert_idx = idx_p[lab]\n",
    "    if insert_idx < width:\n",
    "        sorted_fakes[lab,insert_idx,:,:,:] = x[idx]\n",
    "        idx_p[lab] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45844d1926cd46c7a13264a3cdd39ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe74c3ea0b8>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(sorted_fakes.reshape((10*width,3,x.shape[-1],x.shape[-1]))), nrow=width, padding=0, normalize=True)\n",
    "fake_img = np.moveaxis(fake_grid.data.cpu().numpy(),0,-1)\n",
    "plt.imshow(fake_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "imageio.imsave('/cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/figs/cifar_vanilla_wide_batch.png', fake_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities (CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed609e2746fa48f4b008701624e57672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_out = (np.exp(logit_out.max(1)) / np.exp(logit_out).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas.append(probs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, binso = np.histogram(probs_out, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fef908cc080>]"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax.plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999993230708444"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybins = np.array([0, 0.9, 0.99,0.999,0.9999])\n",
    "x_pos = np.arange(len(mybins))\n",
    "offset = [-0.3, 0, 0.3]\n",
    "for i, d in enumerate(datas):\n",
    "    bt = (np.expand_dims(d,-1) > np.expand_dims(mybins,0)).mean(axis=0)\n",
    "    ax.bar(x_pos + offset[i], bt, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
