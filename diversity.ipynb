{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cfarhomes/ilyak/ilyakavalerov@gmail.com/ramawks69/ACGAN-PyTorch/inception.py:77: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import time\n",
    "import pathlib\n",
    "import imageio\n",
    "import sys\n",
    "import numpy as np\n",
    "import fid\n",
    "import inception as iscore\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "from torchvision.datasets import CIFAR10, STL10\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data as utils\n",
    "import visdom\n",
    "from torchvision import transforms\n",
    "from GAN_training.models import resnet, resnet_extra, resnet_48\n",
    "from tqdm import tqdm\n",
    "\n",
    "import data\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optclass:\n",
    "    workaround = True\n",
    "    \n",
    "    \n",
    "opt = optclass()\n",
    "optdict = {\n",
    "    'outf': '/scratch0/ilya/locDoc/ACGAN/experiments/yogesh_acgan_0p2',\n",
    "    'marygan': False,\n",
    "    'data_root': '/scratch0/ilya/locDoc/data/cifar10',\n",
    "    'dataset': 'cifar',\n",
    "    'dev_batch_size': 100,\n",
    "    'size_labeled_data': 4000,\n",
    "    'train_batch_size': 128,\n",
    "    'train_batch_size_2': 100,\n",
    "    'cifar_fname': '/scratch0/ilya/locDoc/data/cifar10/fid_is_scores.npz',\n",
    "    'nz': 128,\n",
    "    'GAN_nz': 128,\n",
    "    'netG': '',\n",
    "    'imageSize': 32,\n",
    "    'ngpu': 1,\n",
    "    'nc':3\n",
    "}\n",
    "for k, v in optdict.items():\n",
    "    setattr(opt, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/ACGAN/experiments/yogesh_acgan_0p2/netG_iter_274200.pth\n"
     ]
    }
   ],
   "source": [
    "netGfiles = glob.glob(os.path.join(opt.outf, 'netG_iter_*.pth'))\n",
    "netGfiles.sort(key = lambda s: int(s.split('_')[-1].split('.')[0]))\n",
    "opt.netG = netGfiles[-1]\n",
    "print(opt.netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.imageSize == 32:\n",
    "    netG = resnet.Generator(opt)\n",
    "elif opt.imageSize == 64:\n",
    "    netG = resnet_extra.Generator(opt)\n",
    "elif opt.imageSize == 48:\n",
    "    netG = resnet_48.Generator(opt)\n",
    "netG.load_state_dict(torch.load(opt.netG))\n",
    "netG = netG.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/79 [00:00<?, ?it/s]/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "Generating: 100%|██████████| 79/79 [00:07<00:00, 11.29it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = opt.train_batch_size\n",
    "nz = opt.nz\n",
    "noise = torch.FloatTensor(opt.train_batch_size, nz)\n",
    "noise = noise.cuda()\n",
    "num_classes = 10\n",
    "klass_picked = 9\n",
    "\n",
    "# create images\n",
    "n_used_imgs = 10000\n",
    "n_gen_imgs = ((n_used_imgs // opt.train_batch_size) + 1) * opt.train_batch_size\n",
    "x = np.empty((n_gen_imgs,3,opt.imageSize,opt.imageSize), dtype=np.uint8)\n",
    "# create a bunch of GAN images\n",
    "for l in  tqdm(range((n_used_imgs // opt.train_batch_size) + 1),desc='Generating'):\n",
    "    start = l * opt.train_batch_size\n",
    "    end = start + opt.train_batch_size\n",
    "    noise.data.resize_(batch_size, nz).normal_(0, 1)\n",
    "    #label = np.random.randint(0, num_classes, batch_size)\n",
    "    if klass_picked is None:\n",
    "        label = np.random.randint(0, num_classes, batch_size)\n",
    "    else:\n",
    "        label = np.ones((batch_size,),dtype=int)*klass_picked\n",
    "    noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "    if not opt.marygan:\n",
    "        class_onehot = np.zeros((batch_size, num_classes))\n",
    "        class_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
    "    noise_ = (torch.from_numpy(noise_))\n",
    "    noise.data.copy_(noise_.view(batch_size, nz))\n",
    "    fake = netG(noise).data.cpu().numpy()\n",
    "    fake = np.floor((fake + 1) * 255/2.0).astype(np.uint8)\n",
    "    x[start:end] = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb63c8592acb416abb91661339b00a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe45e89af28>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(x), nrow=int(np.sqrt(n_used_imgs)), padding=2, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load feature extracting network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.models.vgg5 import VGG\n",
    "compnet = VGG('VGG19')\n",
    "compnet = torch.nn.DataParallel(compnet)\n",
    "checkpoint = torch.load(os.path.join('/scratch0/ilya/locDoc/classifiers/vgg16','ckpt_200.t7'))\n",
    "compnet.load_state_dict(checkpoint['net'])\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval();\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.models.densenet import DenseNet121\n",
    "compnet = DenseNet121()\n",
    "compnet = torch.nn.DataParallel(compnet)\n",
    "checkpoint = torch.load(os.path.join('/scratch0/ilya/locDoc/classifiers/densenet121','ckpt_47.t7'))\n",
    "compnet.load_state_dict(checkpoint['net'])\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval();\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.models.vgg_official2 import vgg16\n",
    "compnet = vgg16(pretrained=True)\n",
    "compnet = compnet.to(device)\n",
    "compnet.eval()\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass imgs through net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess: 100%|██████████| 10112/10112 [00:01<00:00, 7429.75it/s]\n"
     ]
    }
   ],
   "source": [
    "net_in = np.empty(x.shape)\n",
    "#net_in = np.empty((x.shape[0],) + (3,224,224))\n",
    "for i in tqdm(range(x.shape[0]),desc='Preprocess'):\n",
    "    net_in[i] = transform_test(np.moveaxis(x[i],0,-1))\n",
    "\n",
    "my_dataset = utils.TensorDataset(torch.FloatTensor(net_in))\n",
    "my_dataloader = utils.DataLoader(my_dataset, batch_size=opt.train_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3, 32, 32)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract Feat: 100%|██████████| 79/79 [00:09<00:00,  8.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#net_out = np.empty((x.shape[0], 602112))\n",
    "#net_out = np.empty((x.shape[0], 12288))\n",
    "net_out = np.empty((x.shape[0], 1024))\n",
    "for i, batch in enumerate(tqdm(my_dataloader,desc='Extract Feat')):\n",
    "    start = i * opt.train_batch_size\n",
    "    end = start + opt.train_batch_size\n",
    "    batch_in = batch[0].to(device)\n",
    "    batch_out = compnet(batch_in, True).detach().data.cpu()\n",
    "    net_out[start:end] = batch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE style\n",
    "net_out = x.reshape(x.shape[0], np.prod(x.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10112, 3072)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot closest pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pairwise_distances(net_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the diagonal and lower triangle\n",
    "to_del = np.tril(np.ones((D.shape[0], D.shape[0]), dtype=int))\n",
    "D[to_del == 1] = D.max()\n",
    "\n",
    "\n",
    "dists = D.flatten()\n",
    "closest_N = 20\n",
    "idxs = np.argpartition(dists,closest_N)\n",
    "min_idxs = sorted(idxs[:closest_N], key=lambda i: dists[i])\n",
    "closest_idxs = [(idx // D.shape[0], idx % D.shape[0]) for idx in min_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_imgs = np.empty((closest_N * 2,)+x.shape[1:])\n",
    "for l, (i,j) in enumerate(closest_idxs):\n",
    "    closest_imgs[2*l] = x[min(i,j)]\n",
    "    closest_imgs[2*l + 1] = x[max(i,j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1516d78d257741b4b76bb3151de6094b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe45e353f60>"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot closest pairs\n",
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(closest_imgs[:20]), nrow=2, padding=2, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3711, 9061),\n",
       " (4745, 9061),\n",
       " (3711, 4745),\n",
       " (8305, 9061),\n",
       " (3711, 8305),\n",
       " (2460, 3711),\n",
       " (2460, 9061),\n",
       " (4745, 8305),\n",
       " (2460, 4745),\n",
       " (2460, 8305)]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_idxs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch0/ilya/locDoc/miniconda2/envs/venvcuda10/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf94b67ce5c4233b37ea2ce7caa54ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe45e313518>"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "fake_grid = vutils.make_grid(torch.Tensor(closest_imgs[20:]), nrow=2, padding=2, normalize=True)\n",
    "plt.imshow(np.moveaxis(fake_grid.data.cpu().numpy(),0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5097062616022188"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(((net_out[closest_idxs[0][0]] - net_out[closest_idxs[0][1]])**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 32, 32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(x[closest_idxs[0][0]],0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = transform_test(np.moveaxis(x[closest_idxs[0][0]],0,-1))\n",
    "xb = transform_test(np.moveaxis(x[closest_idxs[0][1]],0,-1))\n",
    "a = compnet(torch.FloatTensor(np.expand_dims(xa,0)).to(device)).detach().data.cpu()\n",
    "b = compnet(torch.FloatTensor(np.expand_dims(xb,0)).to(device)).detach().data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0944, -1.3794,  2.8354,  0.2152, -1.2497,  2.4409,  3.7628, -0.8741,\n",
       "         -1.6860, -2.9780]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
